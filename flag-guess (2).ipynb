{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport os\n\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\nfrom keras import regularizers\nfrom keras.layers import LeakyReLU\nkeras = tf.keras  ","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:54:46.631006Z","iopub.execute_input":"2023-08-02T16:54:46.631427Z","iopub.status.idle":"2023-08-02T16:54:46.638636Z","shell.execute_reply.started":"2023-08-02T16:54:46.631395Z","shell.execute_reply":"2023-08-02T16:54:46.637180Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":266,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.experimental.list_physical_devices('CPU')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:54:46.648286Z","iopub.execute_input":"2023-08-02T16:54:46.649239Z","iopub.status.idle":"2023-08-02T16:54:46.653974Z","shell.execute_reply.started":"2023-08-02T16:54:46.649202Z","shell.execute_reply":"2023-08-02T16:54:46.653010Z"},"trusted":true},"execution_count":267,"outputs":[]},{"cell_type":"code","source":"gpus","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:54:46.666733Z","iopub.execute_input":"2023-08-02T16:54:46.667558Z","iopub.status.idle":"2023-08-02T16:54:46.674410Z","shell.execute_reply.started":"2023-08-02T16:54:46.667523Z","shell.execute_reply":"2023-08-02T16:54:46.673357Z"},"trusted":true},"execution_count":268,"outputs":[{"execution_count":268,"output_type":"execute_result","data":{"text/plain":"[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"},"metadata":{}}]},{"cell_type":"code","source":"gpus = tf.config.experimental.list_physical_devices ('GPU') \nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth (gpu, True)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:54:46.689093Z","iopub.execute_input":"2023-08-02T16:54:46.689385Z","iopub.status.idle":"2023-08-02T16:54:46.694239Z","shell.execute_reply.started":"2023-08-02T16:54:46.689360Z","shell.execute_reply":"2023-08-02T16:54:46.693222Z"},"trusted":true},"execution_count":269,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport imghdr\nfrom matplotlib import pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:54:46.707458Z","iopub.execute_input":"2023-08-02T16:54:46.708050Z","iopub.status.idle":"2023-08-02T16:54:46.712803Z","shell.execute_reply.started":"2023-08-02T16:54:46.708016Z","shell.execute_reply":"2023-08-02T16:54:46.711785Z"},"trusted":true},"execution_count":270,"outputs":[]},{"cell_type":"code","source":"data_dir=r\"/kaggle/input/flag-pictures-dataset/Flags\"","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:54:46.728600Z","iopub.execute_input":"2023-08-02T16:54:46.728900Z","iopub.status.idle":"2023-08-02T16:54:46.732861Z","shell.execute_reply.started":"2023-08-02T16:54:46.728875Z","shell.execute_reply":"2023-08-02T16:54:46.731899Z"},"trusted":true},"execution_count":271,"outputs":[]},{"cell_type":"code","source":"image_exts=[\"jpeg\",\"jpg\",\"bmp\",\"png\"]","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:54:46.747898Z","iopub.execute_input":"2023-08-02T16:54:46.748624Z","iopub.status.idle":"2023-08-02T16:54:46.753322Z","shell.execute_reply.started":"2023-08-02T16:54:46.748597Z","shell.execute_reply":"2023-08-02T16:54:46.751669Z"},"trusted":true},"execution_count":272,"outputs":[]},{"cell_type":"code","source":"num_classes=len(os.listdir(r\"/kaggle/input/flag-pictures-dataset/Flags\"))","metadata":{"execution":{"iopub.status.busy":"2023-08-02T17:26:21.541607Z","iopub.execute_input":"2023-08-02T17:26:21.542060Z","iopub.status.idle":"2023-08-02T17:26:21.551502Z","shell.execute_reply.started":"2023-08-02T17:26:21.542028Z","shell.execute_reply":"2023-08-02T17:26:21.550396Z"},"trusted":true},"execution_count":359,"outputs":[]},{"cell_type":"code","source":"for image_class in os.listdir(data_dir):\n    for image in os.listdir (os.path.join(data_dir, image_class)): \n        image_path = os.path.join(data_dir, image_class, image) \n        try:\n            img= cv2.imread(image_path)\n            tip=imghdr.what (image_path)\n            if tip not in image_exts:\n                print('Image not in ext list {}'.format(image_path))\n                os.remove(image_path)\n        except Exception as e:\n            print('Issue with image {}'.format(image_path))","metadata":{"execution":{"iopub.status.busy":"2023-08-02T17:26:22.715048Z","iopub.execute_input":"2023-08-02T17:26:22.715419Z","iopub.status.idle":"2023-08-02T17:26:46.172587Z","shell.execute_reply.started":"2023-08-02T17:26:22.715390Z","shell.execute_reply":"2023-08-02T17:26:46.171532Z"},"trusted":true},"execution_count":360,"outputs":[{"name":"stderr","text":"libpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: iCCP: known incorrect sRGB profile\nlibpng warning: cHRM: inconsistent chromaticities\nlibpng warning: cHRM: inconsistent chromaticities\nlibpng warning: iCCP: known incorrect sRGB profile\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:55:11.778226Z","iopub.execute_input":"2023-08-02T16:55:11.778873Z","iopub.status.idle":"2023-08-02T16:55:11.783138Z","shell.execute_reply.started":"2023-08-02T16:55:11.778836Z","shell.execute_reply":"2023-08-02T16:55:11.782109Z"},"trusted":true},"execution_count":275,"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n    rescale=1./255,\n    zoom_range=0.2,         # Randomly zoom images by 20%\n    width_shift_range=0.1,  # Randomly shift images horizontally by 10%\n    height_shift_range=0.1, # Randomly shift images vertically by 10%\n    rotation_range=5,      # Randomly rotate images by up to 20 degrees\n    brightness_range=(0.7, 1.3),  # Randomly adjust brightness between 0.8 and 1.2\n    shear_range=0.2,        # Randomly apply shearing transformations\n    horizontal_flip=False,   # Randomly flip images horizontally\n    vertical_flip=False      # Randomly flip images vertically\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:55:11.784845Z","iopub.execute_input":"2023-08-02T16:55:11.785188Z","iopub.status.idle":"2023-08-02T16:55:11.796781Z","shell.execute_reply.started":"2023-08-02T16:55:11.785155Z","shell.execute_reply":"2023-08-02T16:55:11.795779Z"},"trusted":true},"execution_count":276,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = tf.keras.utils.image_dataset_from_directory(\n    data_dir,\n    label_mode='categorical',  # This indicates multiclass classification\n    seed=42,  # Set the seed for reproducibility\n    image_size=(256, 256),\n    batch_size=32\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T17:26:46.174575Z","iopub.execute_input":"2023-08-02T17:26:46.174961Z","iopub.status.idle":"2023-08-02T17:26:46.396297Z","shell.execute_reply.started":"2023-08-02T17:26:46.174914Z","shell.execute_reply":"2023-08-02T17:26:46.395322Z"},"trusted":true},"execution_count":361,"outputs":[{"name":"stdout","text":"Found 3214 files belonging to 18 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"data=data.map(lambda x,y:(x/255,y))","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:55:12.025433Z","iopub.execute_input":"2023-08-02T16:55:12.025788Z","iopub.status.idle":"2023-08-02T16:55:12.042812Z","shell.execute_reply.started":"2023-08-02T16:55:12.025754Z","shell.execute_reply":"2023-08-02T16:55:12.041579Z"},"trusted":true},"execution_count":278,"outputs":[]},{"cell_type":"code","source":"scaled_iterator=data.as_numpy_iterator()","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:55:12.044516Z","iopub.execute_input":"2023-08-02T16:55:12.045105Z","iopub.status.idle":"2023-08-02T16:55:12.067405Z","shell.execute_reply.started":"2023-08-02T16:55:12.045070Z","shell.execute_reply":"2023-08-02T16:55:12.066449Z"},"trusted":true},"execution_count":279,"outputs":[]},{"cell_type":"code","source":"batch=scaled_iterator.next()","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:55:12.068963Z","iopub.execute_input":"2023-08-02T16:55:12.069295Z","iopub.status.idle":"2023-08-02T16:55:12.831840Z","shell.execute_reply.started":"2023-08-02T16:55:12.069262Z","shell.execute_reply":"2023-08-02T16:55:12.830801Z"},"trusted":true},"execution_count":280,"outputs":[]},{"cell_type":"code","source":"train_size = int(len(data) * 0.8)\nval_size = int(len(data) * 0.1)\ntest_size = int(len(data) * 0.1)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:55:12.833419Z","iopub.execute_input":"2023-08-02T16:55:12.833898Z","iopub.status.idle":"2023-08-02T16:55:12.841138Z","shell.execute_reply.started":"2023-08-02T16:55:12.833858Z","shell.execute_reply":"2023-08-02T16:55:12.839921Z"},"trusted":true},"execution_count":281,"outputs":[]},{"cell_type":"code","source":"train=data.take (train_size)\nval = data.skip(train_size).take(val_size) \ntest=data.skip (train_size+val_size).take(test_size)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T17:11:06.676069Z","iopub.execute_input":"2023-08-02T17:11:06.676445Z","iopub.status.idle":"2023-08-02T17:11:06.685016Z","shell.execute_reply.started":"2023-08-02T17:11:06.676416Z","shell.execute_reply":"2023-08-02T17:11:06.684071Z"},"trusted":true},"execution_count":325,"outputs":[]},{"cell_type":"code","source":"# def preprocess_image(image_path):\n#     img = cv2.imread(image_path)\n#     tip = imghdr.what(image_path)\n#     if tip not in image_exts:\n#         print('Image not in ext list {}'.format(image_path))\n#         os.remove(image_path)\n#         return None\n#     # Convert to 8-bit unsigned integers\n#     img = cv2.convertScaleAbs(img)\n#     return img","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:55:12.871498Z","iopub.execute_input":"2023-08-02T16:55:12.872030Z","iopub.status.idle":"2023-08-02T16:55:12.878845Z","shell.execute_reply.started":"2023-08-02T16:55:12.871996Z","shell.execute_reply":"2023-08-02T16:55:12.876693Z"},"trusted":true},"execution_count":283,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Define the data augmentation function\ndef augment_image(image, label):\n    # Randomly adjust brightness, contrast, saturation, and hue\n    image = tf.image.random_brightness(image, max_delta=0.02)\n    image = tf.image.random_contrast(image, lower=0.99, upper=1.01)\n    image = tf.image.random_saturation(image, lower=0.99, upper=1.01)\n    image = tf.image.random_hue(image, max_delta=0.01)\n    # Add more augmentation techniques as per your requirement\n    # For example, you can use tf.image.random_flip_left_right for random horizontal flipping, etc.\n    \n    return image, label\n\n# Define the preprocess function without resizing to use it later for validation and test sets\ndef preprocess_image(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.uint8)  # Convert to 8-bit unsigned int\n    image = tf.image.resize(image, (256, 256))\n    image =image/255.0# Resize to model input size\n    return image, label\n\n# Apply duplication and augmentation to the training data\n# def augment_and_duplicate(image, label, num_duplicates=1):\n#     # List to store augmented images and labels\n#     augmented_images = []\n#     augmented_labels = []\n    \n#     # Add original image and label to the lists\n#     augmented_images.append(image)\n#     augmented_labels.append(label)\n    \n#     # Apply augmentation and add duplicates to the lists\n#     for _ in range(num_duplicates):\n#         augmented_image, augmented_label = augment_image(image, label)\n#         augmented_images.append(augmented_image)\n#         augmented_labels.append(augmented_label)\n    \n#     # Stack the lists to create tensors\n#     augmented_images = tf.stack(augmented_images)\n#     augmented_labels = tf.stack(augmented_labels)\n    \n#     return augmented_images, augmented_labels\n\n# # Duplicate and augment the training data\n# num_duplicates = 0 # You can adjust the number of duplicates as per your requirement\n# train = train.flat_map(lambda image, label: tf.data.Dataset.from_tensor_slices(augment_and_duplicate(image, label, num_duplicates)))\ntrain = train.map(preprocess_image)\n# Apply preprocessing to validation and test sets\nval = val.map(preprocess_image)\ntest = test.map(preprocess_image)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-02T17:26:46.397776Z","iopub.execute_input":"2023-08-02T17:26:46.398123Z","iopub.status.idle":"2023-08-02T17:26:46.446326Z","shell.execute_reply.started":"2023-08-02T17:26:46.398089Z","shell.execute_reply":"2023-08-02T17:26:46.445356Z"},"trusted":true},"execution_count":362,"outputs":[]},{"cell_type":"code","source":"train.as_numpy_iterator().next()[0]","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:58:35.231164Z","iopub.status.idle":"2023-08-02T16:58:35.231627Z","shell.execute_reply.started":"2023-08-02T16:58:35.231385Z","shell.execute_reply":"2023-08-02T16:58:35.231407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=Sequential()","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:55:14.306496Z","iopub.execute_input":"2023-08-02T16:55:14.307341Z","iopub.status.idle":"2023-08-02T16:55:14.322314Z","shell.execute_reply.started":"2023-08-02T16:55:14.307300Z","shell.execute_reply":"2023-08-02T16:55:14.321161Z"},"trusted":true},"execution_count":286,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, (7, 7), 1, activation=keras.layers.LeakyReLU(alpha=0.2), input_shape=(256, 256, 3)))\nmodel.add(MaxPooling2D())\n\n\n# Second convolution layer with increased filters, Dropout, and Regularization\nmodel.add(Conv2D(64, (5, 5), strides=1, activation=keras.layers.LeakyReLU(alpha=0.1), kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling2D())\n\n# Third convolution layer with increased filters and BatchNormalization\nmodel.add(Conv2D(32, (3, 3), strides=1, activation=keras.layers.LeakyReLU(alpha=0.05), kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D())\n\n# Flatten the output to feed into the dense layers\nmodel.add(Flatten())\n\n# Dense layers with Dropout and Regularization\nmodel.add(Dense(256, activation=keras.layers.LeakyReLU(alpha=0.02), kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dropout(0.5))\n\n# Output layer for multiclass classification\nmodel.add(Dense(num_classes, activation='softmax'))\n\nearly_stopping=tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0,\n    patience=1,\n    verbose=1,\n    mode='min',\n    baseline=None,\n    restore_best_weights=True,\n    start_from_epoch=0\n)\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'],)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:55:14.328416Z","iopub.execute_input":"2023-08-02T16:55:14.329191Z","iopub.status.idle":"2023-08-02T16:55:14.491536Z","shell.execute_reply.started":"2023-08-02T16:55:14.329149Z","shell.execute_reply":"2023-08-02T16:55:14.490553Z"},"trusted":true},"execution_count":287,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:55:14.496752Z","iopub.execute_input":"2023-08-02T16:55:14.498948Z","iopub.status.idle":"2023-08-02T16:55:14.554966Z","shell.execute_reply.started":"2023-08-02T16:55:14.498911Z","shell.execute_reply":"2023-08-02T16:55:14.554232Z"},"trusted":true},"execution_count":288,"outputs":[{"name":"stdout","text":"Model: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_9 (Conv2D)           (None, 250, 250, 32)      4736      \n                                                                 \n max_pooling2d_9 (MaxPooling  (None, 125, 125, 32)     0         \n 2D)                                                             \n                                                                 \n conv2d_10 (Conv2D)          (None, 121, 121, 64)      51264     \n                                                                 \n dropout_6 (Dropout)         (None, 121, 121, 64)      0         \n                                                                 \n max_pooling2d_10 (MaxPoolin  (None, 60, 60, 64)       0         \n g2D)                                                            \n                                                                 \n conv2d_11 (Conv2D)          (None, 58, 58, 32)        18464     \n                                                                 \n batch_normalization_3 (Batc  (None, 58, 58, 32)       128       \n hNormalization)                                                 \n                                                                 \n max_pooling2d_11 (MaxPoolin  (None, 29, 29, 32)       0         \n g2D)                                                            \n                                                                 \n flatten_3 (Flatten)         (None, 26912)             0         \n                                                                 \n dense_6 (Dense)             (None, 256)               6889728   \n                                                                 \n dropout_7 (Dropout)         (None, 256)               0         \n                                                                 \n dense_7 (Dense)             (None, 18)                4626      \n                                                                 \n=================================================================\nTotal params: 6,968,946\nTrainable params: 6,968,882\nNon-trainable params: 64\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"logdir='logs'","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:55:14.556077Z","iopub.execute_input":"2023-08-02T16:55:14.556429Z","iopub.status.idle":"2023-08-02T16:55:14.561271Z","shell.execute_reply.started":"2023-08-02T16:55:14.556400Z","shell.execute_reply":"2023-08-02T16:55:14.560530Z"},"trusted":true},"execution_count":289,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:55:14.562562Z","iopub.execute_input":"2023-08-02T16:55:14.562932Z","iopub.status.idle":"2023-08-02T16:55:14.579230Z","shell.execute_reply.started":"2023-08-02T16:55:14.562901Z","shell.execute_reply":"2023-08-02T16:55:14.578486Z"},"trusted":true},"execution_count":290,"outputs":[{"execution_count":290,"output_type":"execute_result","data":{"text/plain":"<_MapDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 18), dtype=tf.float32, name=None))>"},"metadata":{}}]},{"cell_type":"code","source":"tensorboard_callback=tf.keras.callbacks.TensorBoard(log_dir=logdir)\nhist=model.fit(train, epochs=500, validation_data=val, callbacks =[tensorboard_callback,early_stopping])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-08-02T16:55:14.580211Z","iopub.execute_input":"2023-08-02T16:55:14.580887Z","iopub.status.idle":"2023-08-02T16:58:14.261338Z","shell.execute_reply.started":"2023-08-02T16:55:14.580855Z","shell.execute_reply":"2023-08-02T16:58:14.260324Z"},"trusted":true},"execution_count":291,"outputs":[{"name":"stdout","text":"Epoch 1/500\n","output_type":"stream"},{"name":"stderr","text":"2023-08-02 16:55:15.623336: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_7/dropout_6/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"80/80 [==============================] - 36s 407ms/step - loss: 7.5133 - accuracy: 0.5934 - val_loss: 6.4106 - val_accuracy: 0.5562\nEpoch 2/500\n80/80 [==============================] - 24s 296ms/step - loss: 4.7973 - accuracy: 0.7730 - val_loss: 4.6937 - val_accuracy: 0.7375\nEpoch 3/500\n80/80 [==============================] - 24s 288ms/step - loss: 3.1325 - accuracy: 0.8590 - val_loss: 2.9444 - val_accuracy: 0.8562\nEpoch 4/500\n80/80 [==============================] - 24s 299ms/step - loss: 2.2942 - accuracy: 0.8957 - val_loss: 2.4696 - val_accuracy: 0.8656\nEpoch 5/500\n80/80 [==============================] - 34s 416ms/step - loss: 2.1026 - accuracy: 0.9023 - val_loss: 1.9923 - val_accuracy: 0.9031\nEpoch 6/500\n80/80 [==============================] - ETA: 0s - loss: 1.7012 - accuracy: 0.9273Restoring model weights from the end of the best epoch: 5.\n80/80 [==============================] - 24s 294ms/step - loss: 1.7012 - accuracy: 0.9273 - val_loss: 1.7603 - val_accuracy: 0.8875\nEpoch 6: early stopping\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Assuming you have already trained the model and have the test dataset\n# (or you can use 'test' set if you have already created it)\n\n# Get the true labels and predicted labels from the test set\ny_true = []\ny_pred = []\nfor images, labels in test:\n    y_true.extend(np.argmax(labels, axis=1))  # Get the true class indices\n    y_pred.extend(np.argmax(model.predict(images), axis=1))  # Get the predicted class indices\n\n# Compute the confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred)\n\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# If you want a classification report (precision, recall, F1-score, etc.), you can use the following:\nclass_names = os.listdir(r\"/kaggle/input/flag-pictures-dataset/Flags\")\nreport = classification_report(y_true, y_pred, target_names=class_names)\n\nprint(\"Classification Report:\")\nprint(report)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-08-02T16:58:14.262944Z","iopub.execute_input":"2023-08-02T16:58:14.263294Z","iopub.status.idle":"2023-08-02T16:58:34.771798Z","shell.execute_reply.started":"2023-08-02T16:58:14.263258Z","shell.execute_reply":"2023-08-02T16:58:34.770699Z"},"trusted":true},"execution_count":292,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 30ms/step\nConfusion Matrix:\n[[10  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0]\n [ 0 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [ 0  0 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [ 0  0  0 22  0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n [ 0  0  0  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0 26  0  0  0  0  0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0 10  0  0  1  0  0  0  1  0  0  0  0]\n [ 0  0  0  0  0  2  0 19  0  0  0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  2  0  1 27  1  0  0  0  0  0  0  0  2]\n [ 0  0  0  0  0  1  0  0  0 20  0  1  0  0  0  0  0  0]\n [ 0  0  0  0  0  4  0  0  0  0  9  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  3  0 13  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  3 11  0  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0 22  0  0  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 22  0  0  0]\n [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  1  8  0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1 15  0]\n [ 0  0  0  0  0  2  0  0  2  0  1  0  0  0  0  0  0  9]]\nClassification Report:\n              precision    recall  f1-score   support\n\n    Malaysia       1.00      0.83      0.91        12\n      Poland       1.00      1.00      1.00        12\n         USA       1.00      1.00      1.00        14\n    Slovakia       1.00      0.96      0.98        23\n    Paraguay       0.95      1.00      0.97        18\n      Serbia       0.67      1.00      0.80        26\n     Croatia       1.00      0.83      0.91        12\n   Luxemborg       0.95      0.90      0.93        21\n    Thailand       0.93      0.82      0.87        33\n     Belgium       0.77      0.91      0.83        22\n      Latvia       0.90      0.69      0.78        13\n     Liberia       0.76      0.81      0.79        16\n      Brazil       1.00      0.79      0.88        14\n      Canada       0.96      1.00      0.98        22\n      France       0.96      1.00      0.98        22\n   Argentina       0.89      0.80      0.84        10\n Netherlands       1.00      0.94      0.97        16\n     Germany       0.82      0.64      0.72        14\n\n    accuracy                           0.90       320\n   macro avg       0.92      0.88      0.90       320\nweighted avg       0.91      0.90      0.90       320\n\n","output_type":"stream"}]},{"cell_type":"code","source":"new_image","metadata":{"execution":{"iopub.status.busy":"2023-08-02T17:35:51.071520Z","iopub.execute_input":"2023-08-02T17:35:51.072212Z","iopub.status.idle":"2023-08-02T17:35:51.077285Z","shell.execute_reply.started":"2023-08-02T17:35:51.072179Z","shell.execute_reply":"2023-08-02T17:35:51.076151Z"},"trusted":true},"execution_count":384,"outputs":[]},{"cell_type":"code","source":"new_image_path = r\"/kaggle/input/flag-pictures-dataset/Flags/Belgium/belgium-flag-in-grunge-brush-stroke-vector-30915149.jpg\"  # Replace with the path to your new image\n\ni=23\n# Load and preprocess the new image\nnew_image = cv2.imread(new_image_path)\nnew_image = cv2.convertScaleAbs(new_image)\nnew_image = cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR)\nnew_image = tf.image.convert_image_dtype(new_image, dtype=tf.uint8)  \nresize= tf.image.resize(new_image,(256,256))\n# new_image = new_image / 255.0  # Normalize pixel values to [0, 1]\npredictions=model.predict(np.expand_dims(resize,axis=0))\n# Convert the image to a supported depth (e.g., 8-bit unsigned integers)\n\n#  # Add batch dimension\n\n# View the new image\nplt.imshow(new_image.numpy().squeeze())\nplt.axis('off')\nplt.show()\n\npredicted_class_index = np.argmax(predictions[0])\npredicted_class_name = predicted_class_name = sorted(os.listdir(r\"/kaggle/input/flag-pictures-dataset/Flags\"))[predicted_class_index] # Assuming class indices are integers starting from 0\n\n# Print the predicted class index and name\nprint(\"Predicted Class Index:\", predicted_class_index)\nprint(\"Predicted Class Name:\", predicted_class_name)\nprint(\"Predicted Class Name:\", predictions[0][np.argmax(predictions[0])])\npredictions\n","metadata":{"execution":{"iopub.status.busy":"2023-08-02T17:35:27.051091Z","iopub.execute_input":"2023-08-02T17:35:27.051509Z","iopub.status.idle":"2023-08-02T17:35:27.104121Z","shell.execute_reply.started":"2023-08-02T17:35:27.051477Z","shell.execute_reply":"2023-08-02T17:35:27.102700Z"},"trusted":true},"execution_count":381,"outputs":[{"name":"stderr","text":"[ WARN:0@8238.185] global loadsave.cpp:248 findDecoder imread_('https://www.jetpunk.com/img/user-photo-library/15/158a30a34e-450.png'): can't open/read file: check file path/integrity\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[0;32mIn[381], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m new_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(new_image_path)\n\u001b[1;32m      6\u001b[0m new_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mconvertScaleAbs(new_image)\n\u001b[0;32m----> 7\u001b[0m new_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_RGB2BGR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m new_image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mconvert_image_dtype(new_image, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39muint8)  \n\u001b[1;32m      9\u001b[0m resize\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mresize(new_image,(\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m))\n","\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"],"ename":"error","evalue":"OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","output_type":"error"}]},{"cell_type":"code","source":"a=os.listdir(r\"/kaggle/input/flag-pictures-dataset/Flags\")\nsorted(a)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:58:35.152143Z","iopub.execute_input":"2023-08-02T16:58:35.152444Z","iopub.status.idle":"2023-08-02T16:58:35.160477Z","shell.execute_reply.started":"2023-08-02T16:58:35.152417Z","shell.execute_reply":"2023-08-02T16:58:35.159653Z"},"trusted":true},"execution_count":295,"outputs":[{"execution_count":295,"output_type":"execute_result","data":{"text/plain":"['Argentina',\n 'Belgium',\n 'Brazil',\n 'Canada',\n 'Croatia',\n 'France',\n 'Germany',\n 'Latvia',\n 'Liberia',\n 'Luxemborg',\n 'Malaysia',\n 'Netherlands',\n 'Paraguay',\n 'Poland',\n 'Serbia',\n 'Slovakia',\n 'Thailand',\n 'USA']"},"metadata":{}}]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:58:35.161837Z","iopub.execute_input":"2023-08-02T16:58:35.163035Z","iopub.status.idle":"2023-08-02T16:58:35.179407Z","shell.execute_reply.started":"2023-08-02T16:58:35.162997Z","shell.execute_reply":"2023-08-02T16:58:35.178635Z"},"trusted":true},"execution_count":296,"outputs":[{"execution_count":296,"output_type":"execute_result","data":{"text/plain":"array([[2.96494123e-02, 5.20711625e-03, 1.11603245e-01, 1.20886811e-03,\n        2.00817004e-01, 1.39412656e-01, 1.01676829e-04, 3.17499600e-03,\n        8.84200446e-03, 2.32489710e-03, 5.09043515e-04, 1.34732112e-01,\n        1.24749787e-01, 3.16268802e-02, 3.88010615e-03, 7.07147270e-02,\n        1.30066112e-01, 1.37938303e-03]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"\n\nbatch[2][31]","metadata":{"execution":{"iopub.status.busy":"2023-08-02T16:58:35.180443Z","iopub.execute_input":"2023-08-02T16:58:35.180770Z","iopub.status.idle":"2023-08-02T16:58:35.225100Z","shell.execute_reply.started":"2023-08-02T16:58:35.180736Z","shell.execute_reply":"2023-08-02T16:58:35.223532Z"},"trusted":true},"execution_count":297,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[297], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m31\u001b[39m]\n","\u001b[0;31mIndexError\u001b[0m: tuple index out of range"],"ename":"IndexError","evalue":"tuple index out of range","output_type":"error"}]},{"cell_type":"code","source":"new_image","metadata":{"execution":{"iopub.status.busy":"2023-08-02T17:19:12.979004Z","iopub.execute_input":"2023-08-02T17:19:12.979372Z","iopub.status.idle":"2023-08-02T17:19:12.988721Z","shell.execute_reply.started":"2023-08-02T17:19:12.979342Z","shell.execute_reply":"2023-08-02T17:19:12.987499Z"},"trusted":true},"execution_count":338,"outputs":[{"execution_count":338,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(832, 1300, 3), dtype=uint8, numpy=\narray([[[ 56,   0,   0],\n        [ 56,   0,   0],\n        [ 56,   0,   0],\n        ...,\n        [129,   1,   2],\n        [129,   1,   2],\n        [129,   1,   2]],\n\n       [[ 56,   0,   0],\n        [ 56,   0,   0],\n        [ 56,   0,   0],\n        ...,\n        [129,   1,   2],\n        [129,   1,   2],\n        [129,   1,   2]],\n\n       [[ 56,   0,   0],\n        [ 56,   0,   0],\n        [ 56,   0,   0],\n        ...,\n        [129,   1,   2],\n        [129,   1,   2],\n        [129,   1,   2]],\n\n       ...,\n\n       [[  1,   1,   1],\n        [  1,   1,   1],\n        [  1,   1,   1],\n        ...,\n        [  1,   1,   1],\n        [  1,   1,   1],\n        [  1,   1,   1]],\n\n       [[  1,   1,   1],\n        [  1,   1,   1],\n        [  1,   1,   1],\n        ...,\n        [  1,   1,   1],\n        [  1,   1,   1],\n        [  1,   1,   1]],\n\n       [[  1,   1,   1],\n        [  1,   1,   1],\n        [  1,   1,   1],\n        ...,\n        [  1,   1,   1],\n        [  1,   1,   1],\n        [  1,   1,   1]]], dtype=uint8)>"},"metadata":{}}]},{"cell_type":"code","source":"\n\n# Convert the image to a supported depth (e.g., 8-bit unsigned integers)\nimage = cv2.convertScaleAbs((new_image/(255)).numpy(), alpha=(255.0))\nimage = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\nimage = np.expand_dims(image, axis=0)  # Add batch dimension\n\n# View the new image\nplt.imshow(cv2.cvtColor(image.squeeze(), cv2.COLOR_BGR2RGB))\nplt.axis('off')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-02T17:35:38.712527Z","iopub.execute_input":"2023-08-02T17:35:38.713236Z","iopub.status.idle":"2023-08-02T17:35:38.757947Z","shell.execute_reply.started":"2023-08-02T17:35:38.713202Z","shell.execute_reply":"2023-08-02T17:35:38.756668Z"},"trusted":true},"execution_count":382,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[382], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert the image to a supported depth (e.g., 8-bit unsigned integers)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mconvertScaleAbs((\u001b[43mnew_image\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mnumpy(), alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m255.0\u001b[39m))\n\u001b[1;32m      3\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)\n\u001b[1;32m      4\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'NoneType' and 'int'"],"ename":"TypeError","evalue":"unsupported operand type(s) for /: 'NoneType' and 'int'","output_type":"error"}]},{"cell_type":"code","source":"model.save(\"sequential_7\")","metadata":{"execution":{"iopub.status.busy":"2023-08-02T17:40:21.605591Z","iopub.execute_input":"2023-08-02T17:40:21.606568Z","iopub.status.idle":"2023-08-02T17:40:24.406116Z","shell.execute_reply.started":"2023-08-02T17:40:21.606524Z","shell.execute_reply":"2023-08-02T17:40:24.405070Z"},"trusted":true},"execution_count":385,"outputs":[]}]}